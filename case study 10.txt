# ==========================================
# üö¶ Traffic Sign Classification using CNN
# German Traffic Sign Recognition Benchmark
# ==========================================

!pip install tensorflow keras numpy matplotlib scikit-learn opendatasets --quiet

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
import zipfile

# ==========================
# STEP 1: Download dataset
# ==========================
# Dataset: https://www.kaggle.com/datasets/valentynsichkar/traffic-signs-preprocessed
# ‚ö†Ô∏è Requires Kaggle API key (upload kaggle.json)

from google.colab import files
files.upload()  # upload your kaggle.json from Kaggle account

!mkdir ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d valentynsichkar/traffic-signs-preprocessed

# ==========================
# STEP 2: Extract dataset
# ==========================
with zipfile.ZipFile("traffic-signs-preprocessed.zip", 'r') as zip_ref:
    zip_ref.extractall("traffic_sign_dataset")

train_data = np.load('traffic_sign_dataset/train.p')
test_data = np.load('traffic_sign_dataset/test.p', allow_pickle=True)

X_train, y_train = train_data['features'], train_data['labels']
X_test, y_test = test_data['features'], test_data['labels']

print("Train:", X_train.shape, y_train.shape)
print("Test:", X_test.shape, y_test.shape)

# ==========================
# STEP 3: Normalize & split
# ==========================
X_train = X_train / 255.0
X_test = X_test / 255.0

X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)

print("Training:", X_train.shape)
print("Validation:", X_val.shape)
print("Test:", X_test.shape)

# ==========================
# STEP 4: Build CNN model
# ==========================
model = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),
    layers.MaxPooling2D((2,2)),

    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),

    layers.Conv2D(128, (3,3), activation='relu'),
    layers.Flatten(),

    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(43, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# ==========================
# STEP 5: Train model
# ==========================
history = model.fit(
    X_train, y_train,
    epochs=15,
    batch_size=64,
    validation_data=(X_val, y_val)
)

# ==========================
# STEP 6: Evaluate model
# ==========================
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)
print(f"\n‚úÖ Test Accuracy: {test_acc*100:.2f}%")

# ==========================
# STEP 7: Plot training curves
# ==========================
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Accuracy')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Loss')
plt.legend()
plt.show()

# ==========================
# STEP 8: Visualize predictions
# ==========================
predictions = model.predict(X_test)
y_pred = np.argmax(predictions, axis=1)

plt.figure(figsize=(15, 3))
for i in range(10):
    plt.subplot(1, 10, i+1)
    plt.imshow(X_test[i])
    plt.title(f"T:{y_test[i]}\nP:{y_pred[i]}")
    plt.axis('off')
plt.show()
